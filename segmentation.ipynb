{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "from torch.nn import BCELoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Contracting path (Encoder)\n",
    "        # Standard size 128 -> 64 custom size 101 -> 50\n",
    "\n",
    "        self.conv11 = nn.Conv2d(1, 16, kernel_size=3, padding='same')\n",
    "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding='same')\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # Standard size 64 -> 32 custom size 50 -> 25\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding='same')\n",
    "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding='same')\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # Standard size 32 -> 16 custom size 25 -> 12\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
    "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # Standard size 16 -> 8 custom size 12 -> 6\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding='same')\n",
    "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # Middle layer\n",
    "\n",
    "        self.convM1 = nn.Conv2d(128, 256, kernel_size=3, padding='same')\n",
    "        self.convM2 = nn.Conv2d(256, 256, kernel_size=3, padding='same')\n",
    "\n",
    "        # Expanding path (Decoder)\n",
    "        # Standard size 8 -> 16 custom size 6 -> 12\n",
    "\n",
    "        self.upconv5 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv51 = nn.Conv2d(256, 128, kernel_size=3, padding='same')\n",
    "        self.conv52 = nn.Conv2d(128, 128, kernel_size=3, padding='same')\n",
    "\n",
    "        # Standard size 16 -> 32 custom size 12 -> 25\n",
    "\n",
    "        self.upconv6 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv61 = nn.Conv2d(128, 64, kernel_size=3, padding='same')\n",
    "        self.conv62 = nn.Conv2d(64, 64, kernel_size=3, padding='same')\n",
    "\n",
    "        # Standard size 32 -> 64 custom size 25 -> 50\n",
    "\n",
    "        self.upconv7 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.conv71 = nn.Conv2d(64, 32, kernel_size=3, padding='same')\n",
    "        self.conv72 = nn.Conv2d(32, 32, kernel_size=3, padding='same')\n",
    "\n",
    "        # Standard size 64 -> 128 custom size 50 -> 101\n",
    "\n",
    "        self.upconv8 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.conv81 = nn.Conv2d(32, 16, kernel_size=3, padding='same')\n",
    "        self.conv82 = nn.Conv2d(16, 16, kernel_size=3, padding='same')\n",
    "\n",
    "        self.output = nn.Conv2d(16, 1, kernel_size=1, padding='same')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Contracting path (Encoder)\n",
    "        c11 = F.relu(self.conv11(x))\n",
    "        c12 = F.relu(self.conv12(c11))\n",
    "        p1 = self.pool1(c12)\n",
    "\n",
    "        c21 = F.relu(self.conv21(p1))\n",
    "        c22 = F.relu(self.conv22(c21))\n",
    "        p2 = self.pool2(c22)\n",
    "\n",
    "        c31 = F.relu(self.conv31(p2))\n",
    "        c32 = F.relu(self.conv32(c31))\n",
    "        p3 = self.pool3(c32)\n",
    "\n",
    "        c41 = F.relu(self.conv41(p3))\n",
    "        c42 = F.relu(self.conv42(c41))\n",
    "        p4 = self.pool4(c42)\n",
    "\n",
    "        # Middle layer\n",
    "        cM1 = F.relu(self.convM1(p4))\n",
    "        cM2 = F.relu(self.convM2(cM1))\n",
    "\n",
    "        # Expanding path (Decoder)\n",
    "        up5 = self.upconv5(cM2)\n",
    "        concat5 = torch.cat([up5, c42], 1)\n",
    "        c51 = F.relu(self.conv51(concat5))\n",
    "        c52 = F.relu(self.conv52(c51))\n",
    "\n",
    "        up6 = self.upconv6(c52)\n",
    "        concat6 = torch.cat([up6, c32], 1)\n",
    "        c61 = F.relu(self.conv61(concat6))\n",
    "        c62 = F.relu(self.conv62(c61))\n",
    "\n",
    "        up7 = self.upconv7(c62)\n",
    "        concat7 = torch.cat([up7, c22], 1)\n",
    "        c71 = F.relu(self.conv71(concat7))\n",
    "        c72 = F.relu(self.conv72(c71))\n",
    "\n",
    "        up8 = self.upconv8(c72)\n",
    "        concat8 = torch.cat([up8, c12], 1)\n",
    "        c81 = F.relu(self.conv81(concat8))\n",
    "        c82 = F.relu(self.conv82(c81))\n",
    "\n",
    "        output = torch.sigmoid(self.output(c82))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, ids, image_folder, mask_folder, transform=None):\n",
    "        self.ids = ids\n",
    "        self.image_folder = image_folder\n",
    "        self.mask_folder = mask_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, f\"{self.ids[idx]}.png\")\n",
    "        mask_path = os.path.join(self.mask_folder, f\"{self.ids[idx]}.png\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"L\") \n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        #Resize to fit 128x128\n",
    "        target_size=(128, 128)\n",
    "        image = image.resize(target_size, Image.BILINEAR)\n",
    "        mask = mask.resize(target_size, Image.NEAREST)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = './train/images/'\n",
    "train_mask_dir = './train/masks/'\n",
    "train_ids = pd.read_csv(\"./train/train.csv\", usecols=[0]).id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_ids, train_img_dir, train_mask_dir, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 0.4697377383708954, Validation Loss: 0.5601604955196381\n",
      "Epoch 2/20, Training Loss: 0.4372270703315735, Validation Loss: 0.5466133534908295\n",
      "Epoch 3/20, Training Loss: 0.5846647024154663, Validation Loss: 0.5362098984718323\n",
      "Epoch 4/20, Training Loss: 0.3709520995616913, Validation Loss: 1.3747722113132477\n",
      "Epoch 5/20, Training Loss: 0.6054136157035828, Validation Loss: 0.5636618790626526\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "early_stopping_counter = 0\n",
    "early_stopping_patience = 10\n",
    "best_validation_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_masks in train_loader: # We will use the same dataset as the training for ease of use\n",
    "            val_images, val_masks = val_images.to(device), val_masks.to(device)\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_masks)\n",
    "            validation_loss += val_loss.item()\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    validation_loss /= len(train_loader)\n",
    "\n",
    "    # Update learning rate based on validation loss\n",
    "    lr_scheduler.step(validation_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {validation_loss}\")\n",
    "\n",
    "    # Check for improvement in validation loss\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        early_stopping_counter = 0\n",
    "        # Save the model weights\n",
    "        torch.save(model.state_dict(), \"model/bestModel.pth\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    # Check for early stopping\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping: Validation loss has not improved for {} consecutive epochs.\".format(early_stopping_patience))\n",
    "        break\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model/bestModel.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
